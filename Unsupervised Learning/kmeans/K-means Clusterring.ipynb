{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## RUN THIS CELL TO PROPERLY HIGHLIGHT THE HEADER\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Unsupervised models` are trained models using data without labels. Instead of telling an unsupervised algorithm what it should be looking for in the data, the algorithm does the work itself, in a sense independently finding structure within the data.\n",
    "\n",
    "`K-Means clustering` is an unsupervised learning algorithm that, as the name hints, finds a fixed number ($k$) of `clusters` in a dataset. A cluster is a group of data points that are grouped together due to similarities in their features. When using a `K-Means algorithm`, a cluster is defined by a `centroid`, which is a point (either imaginary or real) at the center of a cluster. Every point in a data set is part of the cluster whose centroid is most closely located. To put it simply, the algorithm finds $k$ number of centroids, and then assigns all data points to the closest cluster, with the aim of keeping the centroids small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by randomly defining $k$ `centroids` (can be an actual data point or it could be a random point). From there, the algorithm proceeds as follows:\n",
    "\n",
    "1. Assign each data point to the closest corresponding `centroid`, using the standard [`Euclidean distance`](https://en.wikipedia.org/wiki/Euclidean_distance). \n",
    "2. For each `centroid`, calculate the average of the values of all the points belonging to it. The mean value becomes the new value of the centroid.\n",
    "\n",
    "Once step 2 is complete, all of the centroids have new values that correspond to the average of all of their corresponding points. This process is repeated over and over until there is no change in the centroid values, meaning that they have been accurately grouped. Or, the process can be stopped when a previously determined maximum number of steps has been met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a set of data points as below\n",
    "\n",
    "<img class=\"aligncenter wp-image-3112\" src=\"img/image-1.png\" alt=\"Machine Learning K-Means\" width=\"450\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration 1**: First, we create two randomly generated centroids and assign each data point to the cluster of the closest centroid. In this case, because we are using two centroids, our k value is 2.\n",
    "\n",
    "<img class=\"aligncenter wp-image-3112\" src=\"img/image-2.png\" alt=\"Machine Learning K-Means\" width=\"450\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iteration 2**: As you can see above, the centroids are not evenly distributed. In the second iteration of the algorithm, the average values of each of the two clusters are found and become the new centroid values.\n",
    "\n",
    "<img class=\"aligncenter wp-image-3112\" src=\"img/image-3.png\" alt=\"Machine Learning K-Means\" width=\"450\" height=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterations 3-5**: We repeat the process until there is no further change in the value of the centroids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table><tr>\n",
    "<td> <img class=\"alignleft wp-image-3115\" src=\"img/image-4.png\" alt=\"Machine Learning K-Means\" width=\"350\" height=\"300\"/> </td>\n",
    "<td> <img class=\"alignleft wp-image-3116\" src=\"img/image-5.png\" alt=\"Machine Learning K-Means\" width=\"350\" height=\"300\"/> </td>\n",
    "<td> <img class=\"alignleft wp-image-3117\" src=\"img/image-6.png\" alt=\"Machine Learning K-Means\" width=\"350\" height=\"300\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after iteration 5, there is no further change in the clusters.\n",
    "\n",
    "Now, let's take a look at the visual differences between using $k = 2, 3, 4 \\text{ or }5$ clusters.\n",
    "\n",
    "<p>\n",
    "    <img class=\"alignleft wp-image-3115\" src=\"img/9-and-10-attempt-2.jpg\" alt=\"Machine Learning K-Means\" width=\"690\" height=\"650\">\n",
    "    <img class=\"alignleft wp-image-3117\" src=\"img/10-and-11-attempt-2.jpg\" alt=\"Machine Learning K-Means\" width=\"690\" height=\"650\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing the optimal $K$ number of `clusters` depends on the problem at hand. There are many methods available to do so:\n",
    "\n",
    "* [The Elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)): This is probably the most well-known method for determining the optimal number of clusters. It is also a bit naive in its approach.\n",
    "* [The Silhouette Method](https://en.wikipedia.org/wiki/Silhouette_(clustering)): The silhouette value measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation).\n",
    "\n",
    "`The Elbow Method` is more of a decision rule, while the `Silhouette` is a metric used for validation while clustering. Thus, it can be used in combination with the Elbow Method. Both methods are not alternatives to each other for finding the optimal $K$, rather they are tools to be used together for a more confident decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K-$means steps can be computed as follows:\n",
    "\n",
    "1. Given $N$ data points, $\\{x_1,x_2,\\cdots,x_N\\}$ arrays with real numbers\n",
    "1. Find $K$ cluster centers, $\\{\\mu_1,\\mu_2,...,\\mu_K\\}$\n",
    "1. And assign each data point $x_i$ to one cluster\n",
    "\n",
    "Please note that, we are using the **Euclidean Norm** to calculate distance between two points. You can find out more about it [here](https://machinelearningmastery.com/vector-norms-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Example: Image compression using K-means\n",
    "\n",
    "In our problem of image compression, K-means algorithm will group similar colors together into $K$ clusters of different colors (RGB values). Therefore, each `cluster centroid` becomes the representative of the three dimensional color vector in RGB color space of its respective cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.7.1 (default, Dec 14 2018, 13:28:58) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Numpy version: 1.18.1\n",
      "Matplotlib version: 3.1.0\n"
     ]
    }
   ],
   "source": [
    "## loading relevant libraries\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import seed\n",
    "from numpy.random import randint\n",
    "from numpy.random import choice\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "plt.rcParams['figure.figsize'] = (16, 16)\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n",
    "\n",
    "print('Python version:', sys.version)\n",
    "print('Numpy version:', np.__version__)\n",
    "print('Matplotlib version:', mpl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialising centroids\n",
    "def init_centroid(X, k=5):\n",
    "    seed(2020)\n",
    "    samples = choice(len(X), size=k, replace=False)\n",
    "    return X[samples, :]\n",
    "\n",
    "\n",
    "## Distance calculations (Euclidean distance)\n",
    "def dist(X, C):\n",
    "    return np.linalg.norm(X[:, np.newaxis, :] - C, ord=2, axis=2)**2\n",
    "\n",
    "\n",
    "## Cluster assignment\n",
    "def clst_assign(S):\n",
    "    return np.argmin(S, axis=1)\n",
    "\n",
    "\n",
    "## Update cluster\n",
    "def update(X, clst):\n",
    "    m, d = X.shape\n",
    "    k = max(clst) + 1\n",
    "    C_new = np.empty((k, d))\n",
    "    for v in range(k):\n",
    "        C_new[v, :d] = np.mean(X[clst == v, :], axis=0)\n",
    "    return C_new\n",
    "\n",
    "\n",
    "## Within cluster sum\n",
    "def WithinClusterSSQ(S):\n",
    "    return np.sum(np.amin(S, axis=1))\n",
    "\n",
    "\n",
    "## Applying k-means\n",
    "def kmeans(X, k=5, C_start=None, max_steps=np.inf, breakloop=False):\n",
    "    if C_start is None:\n",
    "        C = init_centroid(X, k)\n",
    "    else:\n",
    "        C = C_start\n",
    "    converged = False\n",
    "    clusters = np.zeros(len(X))\n",
    "    labels = np.zeros(len(X))\n",
    "    i = 1\n",
    "    while (not converged) and (i <= max_steps):\n",
    "        C_old = C\n",
    "        S = dist(X, C)\n",
    "        labels = clst_assign(S)\n",
    "        C = update(X, labels)\n",
    "        converged = np.array_equal(C_old, C)\n",
    "        i += 1\n",
    "        if i > 10 & breakloop == True:\n",
    "            break\n",
    "    return labels, i, WithinClusterSSQ(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we will try to load a color image. An color image is usually stored as an $m \\times n \\times 3$ data array that defines `Red`, `Green`, and `Blue` color components for each individual pixel.\n",
    "\n",
    "<img class=\"aligncenter wp-image-3112\" src=\"img/e91171a3-f7ea-411e-a3e1-6d3892b8e1e5.png\" alt=\"Machine Learning K-Means\" width=\"450\" height=\"350\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('img/donald-trump-april-2020.jpg')\n",
    "np_img = np.array(img, dtype='int32')\n",
    "arr = np_img.astype(dtype='uint8')\n",
    "img = Image.fromarray(arr, 'RGB')\n",
    "plt.figure(figsize=(14,8))\n",
    "imshow(np.asarray(img), aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensions of the matrix is: {} rows x {} columns x depth of {} \".format(\n",
    "        np_img.shape[0], np_img.shape[1], np_img.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way we can view this set of pixels is as a cloud of points in a three-dimensional color space. We will\n",
    "reshape the data to `n_samples x n_features`, and rescale the colors so that they lie between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reshape the data array\n",
    "row, col, l = np_img.shape\n",
    "data = np.reshape(np_img, (row * col, l), order='C')\n",
    "m, k = data.shape\n",
    "print(\"The dimensions of our new data are: {} rows and {} columns\".format(m, k))\n",
    "display(data[:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Centroids initialization with k=5\n",
    "C_inits = init_centroid(data, k=5)\n",
    "print(\"Initial centroids:\\n\", C_inits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Distance calculations\n",
    "M = dist(data, C_inits)\n",
    "print(M[100:104, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cluster Assignment\n",
    "cluster_labels = clst_assign(M)\n",
    "print(\"cluster labels:\", np.unique(cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update centroid values\n",
    "centers = update(data, cluster_labels)\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we may want to do is to find the optimal $K$ value that maximises the _in-between_ cluster distances and minimises the _within_ cluster distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This cell does take quite a bit of time to run.\n",
    "## You may want to put a lower number of clusters to try\n",
    "col_names = ['NumClusters', 'NumIterations', 'WCSSQ']\n",
    "res_df = pd.DataFrame(columns=col_names)\n",
    "for i in range(12):\n",
    "    i += 1\n",
    "    if i % 2 != 0:\n",
    "        continue\n",
    "    new_labels, iteration, WCSSQ = kmeans(data, k=i)\n",
    "    res_df.loc[len(res_df)] = [i, iteration, WCSSQ]\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Plotting the results from the above chunk\n",
    "ax = plt.gca()\n",
    "res_df.plot(style='.-', x='NumClusters', y='WCSSQ', color='blue', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `Elbow-method`, **what is the optimal number of clusters?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let’s have a look at our compressed image\n",
    "new_labels, iteration, WCSSQ = kmeans(data, k=)\n",
    "ind = np.column_stack((data, new_labels))\n",
    "centers = {}\n",
    "for i in set(new_labels):\n",
    "    c = ind[ind[:, 3] == i].mean(axis=0)\n",
    "    centers[i] = c[:3]\n",
    "\n",
    "img_clustered = np.array([centers[i] for i in new_labels])\n",
    "img_disp = np.reshape(img_clustered, (row, col, l), order=\"C\")\n",
    "img_disp = img_disp.astype(dtype='uint8')\n",
    "new_img = Image.fromarray(img_disp, 'RGB')\n",
    "\n",
    "fig = plt.figure(figsize=(16, 16))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "imshow(np.asarray(img))\n",
    "ax1.title.set_text('Original Image')\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "imshow(np.asarray(new_img))\n",
    "ax2.title.set_text('Compressed Image')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: What is your conclusion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s take a look at how we could go about classifying data using the $K$-Means algorithm with `Python`. \n",
    "First thing first, we need to start by importing the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "plt.scatter(X[:,0], X[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcss = []\n",
    "for i in range(1, 15):\n",
    "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "plt.plot(range(1, 15), wcss)\n",
    "plt.title('Elbow Method')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "pred_y = kmeans.fit_predict(X)\n",
    "plt.scatter(X[:,0], X[:,1])\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Little Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume $K = 2$, say we have the following chart\n",
    "\n",
    "<img class=\"alignleft wp-image-3115\" src=\"img/initial_kmeans.png\" alt=\"Machine Learning K-Means\" width=\"690\" height=\"650\">\n",
    "\n",
    "**Question:** After the first iteration, what will be the location of the new centers?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "209px",
    "left": "319px",
    "top": "110px",
    "width": "362px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
