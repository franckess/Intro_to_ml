{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Unsupervised models` are trained models on data without labels. Instead of telling an unsupervised algorithm what it should be looking for in the data, the algorithm does the work itself, in a sense independently finding structure within the data. It is very common in machine learning tasks involving large number of features, that one is advised to use `Principal component analysis` (a.k.a `PCA`). \n",
    "\n",
    "`PCA` is a statistical technique for reducing the number of dimensions in our original dataset whilst retaining most information. It is using the correlation between some dimensions and tries to provide a minimum number of variables that keeps the maximum amount of variation or information about how the original data is distributed. It does not do this using guesswork but using hard mathematics and it uses something known as the `eigenvalues` and `eigenvectors` of the data-matrix.\n",
    "\n",
    "`Eigenvectors` (also known as `principal component`) are basically vectors that are linearly uncorrelated and carry the  variability in our data along their direction.\n",
    "\n",
    "<img class=\"aligncenter wp-image-3112\" src=\"fig/1200px-GaussianScatterPCA.svg.png\" alt=\"PCA\" width=\"650\" height=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PCA`, can be performed in the following steps:\n",
    "\n",
    "* calculate the mean of each column\n",
    "* center the value in each column by subtracting the mean column value\n",
    "* calculate covariance matrix of centered matrix\n",
    "* calculate eigendecomposition of the covariance (eigenvectors represent the magnitude of directions or components for the reduce subspace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematics of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, PCA is implemented as follows:\n",
    "<br>\n",
    "\n",
    "* Given $m$ data points, $\\{x_1,x_2,...,x_m\\} \\in R^n$, with their mean $\\mu = \\frac{1}{m}\\sum_{i=1}^{m}x_i$\n",
    "* Find a direction $w \\in R^n$ where $\\|w\\| \\leq 1$\n",
    "* Such that the variance of the data along the direction $w$ is maximized \n",
    "$$\\max_{w:\\|w\\|\\leq 1}\\underbrace{\\frac{1}{m}\\sum_{i=1}^{m}\\left(w^Tx_i-w^T\\mu\\right)^2}_{\\text{variance}}$$\n",
    "* It can be easily shown that this equals\n",
    "$$w^T\\underbrace{\\left(\\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i-\\mu \\right)\\left(x_i-\\mu \\right)^T\\right)}_{\\text{covariance matrix C}}w = w^TCw$$\n",
    "**So, the optimization problem becomes $$\\max_{w:\\|w\\|\\leq 1}w^TCw$$**\n",
    "* This can be formulated as an eigenvalue problem\n",
    "    * Given a symmetric matrix $C \\in R^{n\\times n}$\n",
    "    * Find a row vector $w \\in R^n$ and $\\|w\\| = 1$\n",
    "    * Such that $Cw = \\lambda w$\n",
    "* There will be multiple solutions of $w_1, w_2, ...$ (eigenvectors) with different $\\lambda_1,\\lambda_2,...$ (eigenvalues)\n",
    "    * They are ortho-normal: $w_i^Tw_i = 1, w_i^Tw_j=0$<br>\n",
    "<br>\n",
    "\n",
    "**To find the top $k$ principal components, first find the mean and covariance matrix from the data $$ \\mu = \\frac{1}{m}\\sum_{i=1}^{m}x_i \\ and \\ C = \\frac{1}{m}\\sum_{i=1}^{m}\\left(x_i-\\mu\\right)\\left(x_i-\\mu\\right)^T$$ calculate the first $k$ eigenvectors $w_1,w_2,...,w_k$ of $C$ corresponding to the largest eigenvalues $\\lambda_1,\\lambda_2,...,\\lambda_k$.**\n",
    "\n",
    "**Then compute the reduced representation \n",
    "$$z_i = \\left(\\begin{split}w_1^T\\left(x_i-\\mu\\right)/\\sqrt{\\lambda_1}\\cr w_2^T\\left(x_i-\\mu\\right)/\\sqrt{\\lambda_2}\\end{split}\\right)$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition Behind PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import eigs\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_all = pd.read_csv('data/food-data.csv', index_col=0,\n",
    "                       sep=',').dropna(axis=0).head(10)\n",
    "\n",
    "display(food_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One-dimensional view (along the 'Real Coffee')\n",
    "food1['y'] = 0\n",
    "\n",
    "x = 'Real coffee'\n",
    "y = 'y'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = sns.regplot(x=x,\n",
    "                 y=y,\n",
    "                 data=food1,\n",
    "                 fit_reg=False,\n",
    "                 marker=\"o\",\n",
    "                 color=\"skyblue\",\n",
    "                 scatter_kws={'s': 350})\n",
    "\n",
    "# add annotations one by one with a loop\n",
    "for i in range(food1.shape[0]):\n",
    "    label = food1.index[i]\n",
    "    x_pos = food1[x][i]\n",
    "    y_pos = food1[y][i]\n",
    "\n",
    "    ax.text(x_pos,\n",
    "            y_pos,\n",
    "            label,\n",
    "            rotation=60,\n",
    "            size='medium',\n",
    "            color='black',\n",
    "            weight='semibold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Two-dimensional view (along the 'Real Coffee' & 'Tea')\n",
    "x = 'Real coffee'\n",
    "y = 'Tea'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax = sns.regplot(x=x,\n",
    "                 y=y,\n",
    "                 data=food_all,\n",
    "                 fit_reg=False,\n",
    "                 marker=\"o\",\n",
    "                 color=\"skyblue\",\n",
    "                 scatter_kws={'s': 350})\n",
    "\n",
    "# add annotations one by one with a loop\n",
    "for i in range(food_all.shape[0]):\n",
    "    label = food_all.index[i]\n",
    "    x_pos = food_all[x][i]\n",
    "    y_pos = food_all[y][i]\n",
    "\n",
    "    ax.text(x_pos,\n",
    "            y_pos,\n",
    "            label,\n",
    "            horizontalalignment='left',\n",
    "            size='medium',\n",
    "            color='black',\n",
    "            weight='semibold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Three-dimensional view (along the 'Real Coffee', 'Tea' & Potatoes)\n",
    "m = food_all[['Real coffee', 'Tea', 'Potatoes']].values\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for i in range(len(m)):\n",
    "    x = m[i, 0]\n",
    "    y = m[i, 1]\n",
    "    z = m[i, 2]\n",
    "    label = i\n",
    "    ax.scatter(x, y, z, c='skyblue', s=60, alpha=1)\n",
    "    ax.text(x,\n",
    "            y,\n",
    "            z,\n",
    "            '%s' % (food_all.index[i]),\n",
    "            horizontalalignment='left',\n",
    "            size='medium',\n",
    "            weight='semibold',\n",
    "            zorder=1,\n",
    "            color='black')\n",
    "\n",
    "ax.set_xlabel('Real coffee')\n",
    "ax.set_ylabel('Tea')\n",
    "ax.set_zlabel('Potatoes')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can we create a scatterplot along all four dimensions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Similarity between NBA players using PCA\n",
    "\n",
    "The dataset `nba.csv` is from the 2017â€“18 NBA Regular season scraped from [Basketball-reference](https://www.basketball-reference.com/leagues/NBA_2018.html) and comprises of traditional NBA statistics (points, rebounds, age, etc) of 466 players.\n",
    "\n",
    "<img class=\"aligncenter wp-image-3112\" src=\"fig/nba_pic.jpg\" alt=\"PCA\" width=\"950\" height=\"650\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "nba_data = pd.read_csv('data/nba.csv', index_col=0, sep=',').dropna(axis=0)\n",
    "# Reset index\n",
    "nba_data.reset_index(drop=True, inplace=True)\n",
    "nba_data.set_index('Player', inplace=True)\n",
    "\n",
    "# Select numeric columns for PCA\n",
    "nba_data = nba_data.select_dtypes(include=[np.number])\n",
    "\n",
    "# dimensions\n",
    "m, k = nba_data.shape\n",
    "\n",
    "print(\"{} x {} table of data:\".format(m, k))\n",
    "display(nba_data.head())\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we normalize the data so each variable has a unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First 250 players (alphabetically)\n",
    "x = nba_data.head(250).values\n",
    "x = x / x.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we estimate the mean for each variable and subtract to center the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_centered = x - x.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we estimate the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = x_centered.T.dot(x_centered) / x_centered.shape[0]\n",
    "print(C[:4, :4])\n",
    "print('...')\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the eigenvectors $w_1, w_2$ of $C$ corresponding to the largest eigenvalue $\\lambda_1$, the second largest eigenvalue $\\lambda_2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2\n",
    "l, W = eigs(C, k=K, v0=np.ones(25))\n",
    "\n",
    "eigenvectors = pd.DataFrame(W.real,\n",
    "                            columns=['Eigenvector 1', 'Eigenvector 2'],\n",
    "                            index=nba_data.columns)\n",
    "\n",
    "eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pd.DataFrame(l.real, index=['Î»1', 'Î»2'])\n",
    "eigenvalues.plot(kind='bar',\n",
    "                 title='First two Eigenvalues',\n",
    "                 rot=0,\n",
    "                 legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's project our original dataset onto the 2-dimensional space we derived above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCs = {\n",
    "    'PC' + str(1 + i): (np.dot(x_centered, W[:, i]) / np.sqrt(l[i])).real\n",
    "    for i in range(W.shape[1])\n",
    "}\n",
    "\n",
    "nba_pca = pd.DataFrame(PCs, index=nba_data.head(250).index)\n",
    "\n",
    "x = 'PC1'\n",
    "y = 'PC2'\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(25, 25))\n",
    "ax = sns.regplot(data=nba_pca,\n",
    "                 x=x,\n",
    "                 y=y,\n",
    "                 fit_reg=False,\n",
    "                 marker=\"o\",\n",
    "                 color=\"skyblue\",\n",
    "                 scatter_kws={'s': 250})\n",
    "\n",
    "# add annotations one by one with a loop\n",
    "for i in range(nba_pca.shape[0]):\n",
    "    label = nba_pca.index[i]\n",
    "    x_pos = nba_pca[x][i] + .06\n",
    "    y_pos = nba_pca[y][i] + .08\n",
    "\n",
    "    ax.text(x_pos,\n",
    "            y_pos,\n",
    "            label,\n",
    "            horizontalalignment='left',\n",
    "            size='large',\n",
    "            color='black',\n",
    "            weight='semibold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also overlay additional information on the chart above. For example, we could overlay the Minutes Played over the entire season (`MP`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "ax = sns.scatterplot(data=nba_pca,\n",
    "                     x=x,\n",
    "                     y=y,\n",
    "                     marker=\"o\",\n",
    "                     s=250,\n",
    "                     hue=(nba_data['MP']),\n",
    "                     ax=ax)\n",
    "\n",
    "for i in range(nba_pca.shape[0]):\n",
    "    label = nba_pca.index[i]\n",
    "    x_pos = nba_pca[x][i] + .06\n",
    "    y_pos = nba_pca[y][i] + .08\n",
    "\n",
    "    ax.text(x_pos,\n",
    "            y_pos,\n",
    "            label,\n",
    "            horizontalalignment='left',\n",
    "            size='medium',\n",
    "            color='black',\n",
    "            weight='semibold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scikit-Learn Example: Handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the [Scikit-Learn implementation of PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# loading dataset\n",
    "digits = load_digits()\n",
    "X = digits.data / 255.0\n",
    "y = digits.target\n",
    "\n",
    "# transforming into a Pandas dataframe\n",
    "feat_cols = ['pixel_' + str(i) for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feat_cols)\n",
    "df['label'] = y\n",
    "df['label'] = df['label'].apply(lambda i: str(i))\n",
    "X, y = None, None\n",
    "\n",
    "print(\"{} x {} table of data:\".format(df.shape[0], df.shape[1]))\n",
    "display(df.head())\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count per label\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rndperm = np.random.permutation(df.shape[0])\n",
    "\n",
    "plt.gray()\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "for i in range(0, 15):\n",
    "    ax = fig.add_subplot(3,\n",
    "                         5,\n",
    "                         i + 1,\n",
    "                         title=\"Digit: {}\".format(\n",
    "                             str(df.loc[rndperm[i], 'label'])))\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.matshow(df.loc[rndperm[i], feat_cols].values.reshape(\n",
    "        (8, 8)).astype(float))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)\n",
    "df['PC1'] = pca_result[:, 0]\n",
    "df['PC2'] = pca_result[:, 1]\n",
    "print('Explained variation per principal component: {}'.format(\n",
    "    pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can do is create a scatterplot of the first and second principal component and color each of the different types of digits with a different color. If we are lucky the same type of digits will be positioned (i.e., clustered) together in groups, which would mean that the first two principal components actually tell us a great deal about the specific types of digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "sns.scatterplot(x=\"PC1\",\n",
    "                y=\"PC2\",\n",
    "                hue=\"label\",\n",
    "                palette=sns.color_palette(\"hls\", 10),\n",
    "                data=df.loc[rndperm, :],\n",
    "                legend=\"full\",\n",
    "                alpha=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
